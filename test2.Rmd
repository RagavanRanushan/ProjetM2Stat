---
title: "test2"
output: html_notebook
---
# Introduction 
## a) Chargement des packages nécessaires
```{r}
# Charger les packages nécessaires
library(ggplot2)
library(readr)
library(tidyr)
library("FactoMineR")
library("factoextra")
library(tidyverse)
library(GGally)
library(FNN)
library(patchwork)
library(caret)
library(lattice)
library(tree)
library(Metrics)
library(dplyr)
library(e1071)
library(randomForest)
library(FNN)
library(pROC)
```


## b) Chargement des données 
```{r}
df <- read_delim("C:/Users/ranus/OneDrive/Bureau/archive(1)/fake_bills.csv")
df
```
les vraiables correspondent à : 

is_genuine : Le billet est-il authentique ? Vrai faux
diagonale : les mesures diagonales en mm
height_left : la hauteur du côté gauche en mm
height_right : la hauteur du côté droit en mm
margin_low : la marge inférieure en mm
margin_up : la marge supérieure en mm
longueur : la longueur en mm





# A) Analyse descriptive 

## 1) Valeurs manquantes

On determine ici le nombres de valeurs manquantes.
```{r}
sum(is.na(df))
heatmap(matrix(as.numeric(is.na(df)),ncol = 7))
```

Il y a donc dans l'ensemble du data set, 37 valeurs manquantes. 

Il faut ensuite déterminer leurs localisations. 

```{r}
count(df,margin_low,sort = TRUE)
```
On remarque que les 37 valeurs manquantes sont toutes présentes au même endroit, elles sont toutes dans la variable margin_low.

Il existe plusieurs méthodes pour remplacer les données manquantes 
- on peut remplacer par la moyenne 
- on peut remplacer par la medianne 
- on peut remplacer par la catégorie dominante
- on peut créer une nouvelle catégorie 
- on peut faire de la régression linéaire 
- on peut faire du k-mean 


```{r}
cor(df)
```

On ne peut pas se prononcer sur les corrélation linéaire entre margin_low et les autres variables.
Cependant on remarque que is_genuine est fortement corrélé avec length et anti corrélé avec margin_up

### a) Régression linéaire 

Comme vu précemment l'une des variables quantitative possède des valeurs manquantes. Nous allons utiliser une méthode de computation via GLM pour les remplacer.
On commencer par isoler les données manquantes, puis nous retirons la variable target pour éviter d'utiliser les variables target pour l'imputation et biaisé la suite de l'étude.

```{r}
df%>%
  ggplot()+geom_histogram(aes(x=margin_low,alpha=0.05))
```

```{r}
df_na=df[is.na(df$margin_low),]
df_no_na=na.omit(df)
df_no_na
target_no_na=df_no_na$is_genuine
df_2_no_na=df_no_na[,-1]
```

#### - critère AIC
```{r}
#target=df$is_genuine
#data=df[,-1]
```

```{r}
mod0<-glm(margin_low~1,data=df_2_no_na,family='gaussian')
mod1<-glm(margin_low~.,data=df_2_no_na,family='gaussian')
stp<-step(mod0,scope=list(lower=mod0,upper=mod1),direction='both')
summary(stp)
```
Le meilleur modèle est celui qui minimise l'AIC cet à dire celui de la forme $$ margin_low ~ length + height_right + margin_up + 
    height_left + diagonal $$


#### - critère BIC
```{r}
mod0<-glm(margin_low~1,data=df_2_no_na,family=gaussian)
mod1<-glm(margin_low~.,data=df_2_no_na,family=gaussian)
stp2<-step(mod0,scope=list(lower=mod0,upper=mod1),direction='both',k=log(nrow(df_2_no_na)))
summary(stp2)
```
Le meilleur modèle est celui qui minimise l'AIC cet à dire celui de la forme $$ margin_low ~ length + height_right + margin_up + 
    height_left + diagonal $$. Nous n'avonse pas utiliser la variable target pour l'imputation pour éviter d'influencer les futures prédictions.

```{r}


mod3<-glm(margin_low~.,data=df_no_na[-1],family=gaussian)

#On prédit les valeurs manquante avec le glm
imputation_value=predict(mod3,df_na[,-1])

#on remplace la colonne d'origine par la prediction, puis nous fusionnons les observtion sans na et celles qui en avaient
df_na$margin_low=imputation_value #remplacement

df_clean=rbind(df_no_na,df_na)#fusion 

set.seed(42)  # melange des samples
rows <- sample(nrow(df_clean))
data <- as.data.frame(df_clean[rows, ])
data
```


## 2) Visualisation
### a) Pairplot
```{r,warning=FALSE}
ggpairs(data =data ,
        columns = 2:7,
        title = "visualisation des variables",
        upper = list(continuous = wrap("density", alpha = 0.75), combo = "box_no_facet"),
        lower = list(continuous = wrap("points", alpha = 0.5, size=0.1), combo = "facethist"),
        diag = list(continuous = wrap("densityDiag", alpha = 0.5)),
        progress = FALSE,
        aes(colour=is_genuine))
```

```{r,warning=FALSE}
```

On a représenté l'ensembles des variables en fonctions d'elle même en faisant la distinction de couleur sur l'authenticité des billets. 

On obtient une matrice de graphs, elle est composé de trois parties distincts : 
- le triangle supérieur :
  - 
- la diagonale :
  - la densité des variables conditionnellement à leur authenticité 
- le triangle inférieur 
  - 

Mise à part la variable diagonal, les densités conditionnelle de toutes les variables sont différentes

-On remarque également une séparation entre les deux deux classe pour la variable length. Ainsi les modèles avec une séparation linéaire pourrait donner de bon resultat.

### b) Boxplot
```{r,,warning=FALSE}
ggplot(data)+geom_boxplot(x="is_genuine",y="height_left")->p
p1 <- ggplot(data, aes(x=is_genuine, y=height_left,fill=is_genuine)) + 
  geom_boxplot()

p2 <- ggplot(data, aes(x=is_genuine, y=height_right,fill=is_genuine)) + 
  geom_boxplot()

p3 <- ggplot(data, aes(x=is_genuine, y=diagonal,fill=is_genuine)) + 
  geom_boxplot()

p4 <- ggplot(data, aes(x=is_genuine, y=margin_low,fill=is_genuine)) + 
  geom_boxplot()

p5 <- ggplot(data, aes(x=is_genuine, y=margin_up,fill=is_genuine)) + 
  geom_boxplot()

p6 <- ggplot(data, aes(x=is_genuine, y=length,fill=is_genuine)) + 
  geom_boxplot()

(p1+p2+p3)/(p4+p5+p6)
```

On remarque de nouveau qu'à l'exception de diagonal, toutes les variables ont un impact sur la variable is_genuine. 

On remarque la présence d'outliers sur toutes les variables. Les modèles paramétrique y sont sensible, ainsi dans l'éventualité ou  nous utiliseront des modèle paramétriques il faudra envisager de standardiser nos variable ou d'appliquer log dessus.

Les modèles non paramétrique comme les arbres ou le knn sont robuste face aux outliers.

De plus margin_low,legnth et height_left semble avoir des variances différentes.
Verifions cette hypothèse avec un test de bartlet
```{r}
bartlett.test(data$margin_low,data$is_genuine)
```

```{r}
bartlett.test(data$height_left,data$is_genuine)
```

```{r}
bartlett.test(data$length,data$is_genuine)
```
On peut rejeter l'hypothèse d'homogéinité des variance.

```{r}
loga<-function(data){
  n=ncol(data)  
  for(i in 2:n){
    data[,i]=log(data[,i])
  }
  return(data)
}
log_data=loga(data)
```


```{r}

bartlett.test(log_data$margin_low,log_data$is_genuine)
reg1<-lm(margin_low~is_genuine,log_data)
anova(reg1)
```
```{r}
bartlett.test(log_data$height_left,log_data$is_genuine)
reg2<-lm(height_left~is_genuine,log_data)
anova(reg2)
```

```{r}
bartlett.test(log_data$length,log_data$is_genuine)
reg3<-lm(length~is_genuine,log_data)
anova(reg3)
```

On remarque également la présence d'outlier pour toutes les variables.

Deplus la présence d'outliers va guider le choix des modèles à utiliser.

Les modèles composé d'arbre de decision sont robuste aux outliers contrairement aux modèles linéaires.




##3) Valeurs aberrantes
###a) logarithme/standardiser (minimiser l'impacte de ces valeurs)
```{r}

```

###b) choix du modèle vis à vis des valeurs abhérentes
```{r}

```






##4) rééchantillonnage 
(parce qu'il y a soit VRAI soit FAUX et les FAUX sont au nombre de 500 sur les 1500 données)

Etant donnée le faible nombre d'observation, nous allons plutôt opter pour une méthode d'oversampling pour remédier au problème.

Pour cela nous allons faire du boostrapping, cependant il faudra faire attention car cette méthode favorise le surajustement.

```{r}
data_na=data[data$is_genuine==FALSE,]
resample=sample(x = 1:500,size = 400,replace=T)

re_data=data_na[resample,]

data2=rbind(data,re_data)


set.seed(43)  # melange des samples
rows <- sample(nrow(data2))
data2 <- as.data.frame(data2[rows, ])
data2
```

```{r}
#write.csv(data, "C:/Users/ranus/OneDrive/Bureau/DataScience/ProjetM2Stat/Data.csv", row.names=FALSE)
```








#B) Modèles de classifications 
##1) Régression logistique 



cor(df)
###a) hyperparamètres / Cross-validation


```{r}
#encodage de la variable target
data2$is_genuine=as.factor(as.integer(data2$is_genuine))
data$is_genuine=as.factor(as.integer(data$is_genuine))

# division des donnée en donnée train et donnée test
test_row=sample(nrow(data)*0.25,replace = FALSE)
data_train=data[-test_row,]
data_test=data[test_row,]
```

Après encodage de la variable cible, on a:
-0=False
-1=True

Nous allons chercher à minimiser les erreurs sur les faux positif cet à dire diminuer le risque de dire qu'un faux billet est un vrai. Nous allons donc utiliser la métrique recall.

Nous utiliserons également la métrique précision pouur évaluer l'erreur faites en affirmant qu'un billet est faux alors qu'il est vrai.

Et enfin on utilisera la métrique F1 qui combine les deux.

```{r}


log_reg=glm(is_genuine ~ .,data=data_train,family = binomial)
as.data.frame(as.vector(predict(log_reg,data_test,type='respons')))

proba_rl=as.vector(predict(log_reg,data_test,type='respons'))

pred_rl=replicate(length(proba_rl),0)

for(i in 1: length(proba_rl)){
  if(proba_rl[i]>0.5){
    pred_rl[i]=1
  }
  else{
    pred_rl[i]=0
  }
}
pred_rl
```

###b) Métriques d'évaluation
```{r}
x<-c("modele","accuracy","f1","recall","precision")
rl_metric<-cbind(
  as.data.frame(c("regression logistique")),
  as.data.frame(accuracy(data_test$is_genuine,pred_rl)),
  as.data.frame(f1(data_test$is_genuine,pred_rl)),
  as.data.frame(recall(data_test$is_genuine,pred_rl)),
  precision(as.numeric(as.vector(data_test$is_genuine)),as.numeric((pred_rl))))


colnames(tree_metric) <- x
```
```{r}
rocobj <- roc(as.numeric(data_test$is_genuine), as.numeric(pred_rl))


ggroc(rocobj)

auc(as.numeric(data_test$is_genuine), as.numeric(pred_rl))
```

```{r}
auc(as.numeric(data_test$is_genuine), as.numeric(pred_rl))
```


##2) Arbre de classification
###a) hyperparamètres / Cross-validation
```{r}
#refaire arbre pour diagonal + height_left et un pour diagonal + length
tr<-tree(is_genuine~.,data=data_train,control=tree.control(nobs=nrow(data_train),mincut=5,minsize=10))

y_hat_tr<-predict(tr,newdata=data_test)



```
On obtient des proba. On ne va conserver que les valeurs avec la plus grande proba

```{r}
n=nrow(y_hat_tr)
y=replicate(n,0)
for(i in 1:n){
  if(y_hat_tr[i,2]>y_hat_tr[i,1]){
    y[i]=1
  }
}
y_hat_tr=y
```
La table de contingence nous montre de très bon resultat pour l'arbre.
```{r}

table(data_test$is_genuine,y_hat_tr)
```
```{r}
cbind(data_test,y_hat_tr)%>%
  ggplot(aes(x=height_left,y=margin_low,color=is_genuine,shape=as.factor(y_hat_tr)))+geom_point()

```



###b) Métriques d'évaluation
```{r}
x<-c("modele","accuracy","f1","recall","precision")
tree_metric<-cbind(
  as.data.frame(c("tree")),
  as.data.frame(accuracy(data_test$is_genuine,y_hat_tr)),
  as.data.frame(f1(data_test$is_genuine,y_hat_tr)),
  as.data.frame(recall(data_test$is_genuine,y_hat_tr)),
  as.data.frame(precision(as.numeric(as.vector(data_test$is_genuine)),as.numeric(as.vector(pred)))))


colnames(tree_metric) <- x
tree_metric
```

```{r}
table(data_test$is_genuine,y_hat_tr)


```

```{r}
rocobj <- roc(as.numeric(data_test$is_genuine), as.numeric(y_hat_tr))


ggroc(rocobj)
```
```{r}
auc(as.numeric(data_test$is_genuine), as.numeric(y_hat_tr))
```

##3) SVM 
###a) hyperparamètres / Cross-validation


```{r}
k=5
kernels<-c("linear","radial basis","sigmoid",'polynomial')

for(ker in kernels){

  n=nrow(data_train)
  valid_size=n%/%k
  for (i in 1:k){
    d=valid_size*(i-1)
    e=valid_size*i
    valid_data=data_train[c(d:e),]
    data_train2=data_train[-c(d:e),]
    
    svmfit = svm(is_genuine ~ ., data = data_train2, kernel = kernels, cost = 10, scale = FALSE)
    pred<-predict(svmfit,newdata=valid_data)
    print(paste0('model: SVM  kernel:',ker, "  precision: " ,precision(as.numeric(as.vector(valid_data$is_genuine)),as.numeric(as.vector(pred))),"  recall:", recall(as.numeric(as.vector(valid_data$is_genuine)),as.numeric(as.vector(pred)))))
  }
 print( paste0(" "))
 print( paste0(" "))
 print( paste0(" "))
 print( paste0(" "))
  }
```

Il semblerait que le meilleur noyeau est le sigmoid.

```{r}
svmfit = svm(is_genuine ~ ., data = data_train2, kernel = "linear", cost = 10, scale = FALSE)
y_hat_svm<-predict(svmfit,newdata=data_test)
```


###b) Métriques d'évaluation
```{r}
svm_metric<-cbind(
  as.data.frame(c("svm")),
  as.data.frame(accuracy(data_test$is_genuine,y_hat_svm)),
  as.data.frame(f1(data_test$is_genuine,y_hat_svm)),
  as.data.frame(recall(as.numeric(as.vector(data_test$is_genuine)),as.numeric(as.vector(y_hat_svm)))),
  as.data.frame(precision(as.numeric(as.vector(data_test$is_genuine)),as.numeric(as.vector(y_hat_svm)))))

colnames(svm_metric) <- x
svm_metric
```

```{r}
auc(as.numeric(data_test$is_genuine), as.numeric(y_hat_svm))
```


table de contingence
```{r}


table(y_hat_svm,data_test$is_genuine)
```



```{r}
rocobj <- roc(as.numeric(data_test$is_genuine), as.numeric(y_hat_svm))


ggroc(rocobj)
```

Il est normal d'avoir une meilleure performence avec le kernel linéaire car nous avons vu plus haut que la variable cible avait une belle séparation linéaire pour ceertaine variable.

Si nous avions choisie un autre kernel, nous aurions eu des resultats nettement moins bon. Ci dessous on peu voir la courbe pour le SVM avec un kernel sigmoid.


```{r}
svmfit = svm(is_genuine ~ ., data = data_train2, kernel = "sigmoid", cost = 10, scale = FALSE)
y_hat_svm<-predict(svmfit,newdata=data_test)

rocobj <- roc(as.numeric(data_test$is_genuine), as.numeric(y_hat_svm))


ggroc(rocobj)

```
On remarque que la courbe ROC du SVM sigmoid correspond à la 1er bissectrice, ce qui indique que le modèle assigne  aléatoirement les classes pendant la prédiction.

##4) ACP 

Nous pouvons également considérer notre problème comme un problème de clustering.

###a) hyperparamètres / Cross-validation
```{r}
pca=PCA(X = data2[,-1],scale.unit = T)

fviz_pca_ind(pca,
             geom.ind = "point", # Montre les points seulement (mais pas le "text")
             col.ind = data2$is_genuine, # colorer by groups
             palette = c("#00AFBB", "#E7B800"),
             addEllipses = TRUE, # Ellipses de concentration
             legend.title = "Groups"
             )
```

On remarque que la deuxième composante de l'acp explique très peu la variable is_genuine. De plus cette composante est fortement corréler à la variable diagonal
qui est très faiblement impacté par la variable target.

On remarque également que la première composante explique bien la variabe cible. La classe 1 lui étant anti corréler tandis que la classe 0 est corréler.

De plus toutes les autres variable lui sont correler/anti corréler avec.

###b) Métriques d'évaluation
```{r}

```

L'ACP fait tout de même des erreurs et commet plus d'erreur que les autres méthodes.

##5) Forêt aléatoire  
###a) hyperparamètres / Cross-validation

```{r}
k=5
ntrees=c(10,50,100)
mtrys=c(1,2,3)

for(ntree in ntrees){
  for(mtry in mtrys){

  n=nrow(data_train)
  valid_size=n%/%k
  for (i in 1:k){
    d=valid_size*(i-1)
    e=valid_size*i
    valid_data=data_train[c(d:e),]
    data_train2=data_train[-c(d:e),]
    
    rf<-randomForest(is_genuine~.,data=data_train2,mtry=mtry,ntree=ntree)
    pred<-predict(rf,newdata=valid_data)
    print(paste0('model: RandomForest  mtry:',mtry,' ntree: ',ntree, "  precision: " ,precision(as.numeric(as.vector(valid_data$is_genuine)),as.numeric(as.vector(pred))),"  recall:", recall(as.numeric(as.vector(valid_data$is_genuine)),as.numeric(as.vector(pred)))))
  }
 print( paste0(" "))
 print( paste0(" "))
 print( paste0(" "))
 print( paste0(" "))
}}

```

GridSearch sur le nombre de variable utilisé a chaque split mtry et le nombres d'arbres
mtry={1,2,3} et ntree={10,50,100} et une cross validation du type 5Fold et

Meilleurs modèle
mtry:1 ntree: 50



###b) Métriques d'évaluation
```{r}
rf<-randomForest(is_genuine~.,data=data_train,mtry=1,ntree=50)
y_hat_rf<-predict(rf,data_test)
rf_metric=cbind(
  as.data.frame(c("random Forest")),
  as.data.frame(accuracy(data_test$is_genuine,y_hat_rf)),
  as.data.frame(f1(data_test$is_genuine,y_hat_rf)),
 as.data.frame(recall(as.numeric(as.vector(data_test$is_genuine)),as.numeric(as.vector(y_hat_rf)))),
  as.data.frame(precision(as.numeric(as.vector(data_test$is_genuine)),as.numeric(as.vector(y_hat_rf)))))

colnames(rf_metric) <- x
rf_metric
```
```{r}
rocobj <- roc(as.numeric(data_test$is_genuine), as.numeric(y_hat_rf))


ggroc(rocobj)
```


```{r}
auc(as.numeric(as.vector(data_test$is_genuine)),as.numeric(as.vector(y_hat_rf)))
```


##6) KNN
###a) hyperparamètres / Cross-validation

GridSearch sur le paramètre k
```{r}

k=5
Kcluster=c(1:5)

for(K in Kcluster){

  n=nrow(data_train)
  valid_size=n%/%k
  for (i in 1:k){
    d=valid_size*(i-1)
    e=valid_size*i
    valid_data=data_train[c(d:e),]
    data_train2=data_train[-c(d:e),]
    
    pred<-knn(train=as.matrix(data_train2[,-1]),test=as.matrix(valid_data[,-1]),cl =data_train2$is_genuine,k=K)
    
    print(paste0('model: knn  k:',K, "  precision: " ,precision(as.numeric(as.vector(valid_data$is_genuine)),as.numeric(as.vector(pred))),"  recall:", recall(as.numeric(as.vector(valid_data$is_genuine)),as.numeric(as.vector(pred)))))
  }
 print( paste0(" "))
 print( paste0(" "))
 print( paste0(" "))
 print( paste0(" "))
}



knn2<-knn(train=as.matrix(data_train[,-1]),test=as.matrix(data_test[,-1]),cl =data_train$is_genuine,k=2)
```

Il semblerait que le meilleurs k soit 3.
Cependant la methode elbow serait peut etre plus adéquat pour trouver le k optimal.






###b) Métriques d'évaluation

```{r}

knn2<-knn(train=as.matrix(data_train[,-1]),test=as.matrix(data_test[,-1]),cl =data_train$is_genuine,k=3)
accuracy(actual = data_test$is_genuine,predicted = knn2)
precision(actual = data_test$is_genuine,predicted = knn2)
```

```{r}
knn_metric<-cbind(
  as.data.frame(c("knn")),
  as.data.frame(accuracy(data_test$is_genuine,knn2)),
  as.data.frame(f1(as.numeric(as.vector(data_test$is_genuine)),as.numeric(as.vector(knn2)))),
  as.data.frame(recall(as.numeric(as.vector(data_test$is_genuine)),as.numeric(as.vector(knn2)))),
  as.data.frame(precision(as.numeric(as.vector(data_test$is_genuine)),as.numeric(as.vector(knn2)))))


colnames(knn_metric) <- x
knn_metric
```

table de contingence
```{r}
table(as.vector(data_test$is_genuine),as.vector(knn2))
```


```{r}
rocobj <- roc(as.numeric(data_test$is_genuine), as.numeric(knn2))


ggroc(rocobj)
```

```{r}
auc(as.numeric(data_test$is_genuine), as.numeric(knn2))
```




#C) Conclusion

Si nous cherchons à minimiser l'erreur faites sur qui est faites en déclarant qu'un billet est vrai alors qu'il est faux le meilleurs des modèles tester est....

Si au contraire on cherche plutôt à minimiser le fait de déclarer qu'on déclare un billet faux alors qu'il est vrai on priviligiera le modèle....

Enfin si on cherche un modèle équilibré, on prendra...




#D) Bibliographie

https://stackoverflow.com/questions/19400494/running-a-stepwise-linear-model-with-bic-criterion


```{r}
actual <- c(1, 1, 1, 0, 0, 0)
predicted <- c(1, 1, 1, 1, 1, 1)
table(actual, predicted)
```






```{r}
k=5
ntrees=c(10,50,100)
mtrys=c(1,2,3)

for(ntree in ntrees){
  for(mtry in mtrys){

  n=nrow(data_train)
  valid_size=n%/%k
  for (i in 1:k){
    d=valid_size*(i-1)
    e=valid_size*i
    valid_data=data_train[c(d:e),]
    data_train2=data_train[-c(d:e),]
    
    rf<-randomForest(is_genuine~.,data=data_train2,mtry=mtry,ntree=ntree)
    pred<-predict(rf,newdata=valid_data)
    print(paste0('model: RandomForest  mtry:',mtry,' ntree: ',ntree, "  precision: " ,precision(as.numeric(as.vector(valid_data$is_genuine)),as.numeric(as.vector(pred))),"  recall:", recall(as.numeric(as.vector(valid_data$is_genuine)),as.numeric(as.vector(pred)))))
  }
 print( paste0(" "))
 print( paste0(" "))
 print( paste0(" "))
 print( paste0(" "))
}}

```
GridSearch sur le nombre de variable utilisé a chaque split mtry et le nombres d'arbres
mtry={1,2,3} et ntree={10,50,100} et une cross validation du type 5Fold et

Meilleurs modèle
mtry:1 ntree: 50


```{r}
k=5
kernels<-c("linear","radial basis","sigmoid",'polynomial')

for(ker in kernels){

  n=nrow(data_train)
  valid_size=n%/%k
  for (i in 1:k){
    d=valid_size*(i-1)
    e=valid_size*i
    valid_data=data_train[c(d:e),]
    data_train2=data_train[-c(d:e),]
    
    svmfit = svm(is_genuine ~ ., data = data_train2, kernel = kernels, cost = 10, scale = FALSE)
    pred<-predict(svmfit,newdata=valid_data)
    print(paste0('model: SVM  kernel:',ker, "  precision: " ,precision(as.numeric(as.vector(valid_data$is_genuine)),as.numeric(as.vector(pred))),"  recall:", recall(as.numeric(as.vector(valid_data$is_genuine)),as.numeric(as.vector(pred)))))
  }
 print( paste0(" "))
 print( paste0(" "))
 print( paste0(" "))
 print( paste0(" "))
  }
```

Il semblerait que le meilleur noyeau est le linéaire.


```{r}

k=5
Kcluster=c(1:7)

for(K in Kcluster){

  n=nrow(data_train)
  valid_size=n%/%k
  for (i in 1:k){
    d=valid_size*(i-1)
    e=valid_size*i
    valid_data=data_train[c(d:e),]
    data_train2=data_train[-c(d:e),]
    
    pred<-knn(train=as.matrix(data_train2[,-1]),test=as.matrix(valid_data[,-1]),cl =data_train2$is_genuine,k=K)
    
    print(paste0('model: knn  k:',K, "  precision: " ,precision(as.numeric(as.vector(valid_data$is_genuine)),as.numeric(as.vector(pred))),"  recall:", recall(as.numeric(as.vector(valid_data$is_genuine)),as.numeric(as.vector(pred)))))
  }
 print( paste0(" "))
 print( paste0(" "))
 print( paste0(" "))
 print( paste0(" "))
}



knn2<-knn(train=as.matrix(data_train[,-1]),test=as.matrix(data_test[,-1]),cl =data_train$is_genuine,k=2)
```

Il semblerait que le meilleurs k soit 7.
La methode elbow nous indique que le meilleur k est 7. Ce point est confirmer par une cross validation.




```{r}
valid_size=nrow(data_train)*0.25
valid_data=data_train[c(1:valid_size),]
data_train2=data_train[-c(1:valid_size),]
X=replicate(10,0)
for(k in c(1:10)){
  
  pred<-knn(train=as.matrix(data_train2[,-1]),test=as.matrix(valid_data[,-1]),cl =data_train2$is_genuine,k=k)
  #X[k]=recall(as.numeric(as.vector(valid_data$is_genuine)),as.numeric(as.vector(pred)))
  X[k]=mean(pred!=valid_data$is_genuine)
}

as.data.frame(cbind(c(1:10),X))%>%
  ggplot(aes(x=V1,y=X))+geom_line()
```

